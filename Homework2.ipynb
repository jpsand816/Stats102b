{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43746a78-600a-4496-a29c-128d8e4c6092",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153a28dc-bae0-459a-abaf-7c1a49846fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import time\n",
    "np.random.seed(606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436aec7d-adaf-454b-8b44-10bec3b39a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset-logistic-regression.csv\")\n",
    "\n",
    "X = np.array(df.iloc[:, 1:])\n",
    "y = np.array(df[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f68176-4a43-4634-8775-928794f303b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    #denom = 1 + np.exp(-z)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#error functions\n",
    "def error(X, y, beta):\n",
    "    pred = np.clip(sigmoid(X @ beta), 1e-10, 1 - 1e-10)\n",
    "    return -np.mean(y * np.log(pred) + (1 - y) * np.log(pred))\n",
    "\n",
    "def grad_error(X, y, beta):\n",
    "    pred = sigmoid(X @ beta)\n",
    "    return (1 / X.shape[0]) * X.T @ (pred - y)\n",
    "    \n",
    "\n",
    "#SGD Implementation\n",
    "def error_SGD(X, y, w, n):\n",
    "    left = np.clip(y[n] * np.log(sigmoid(w.T @ X[n])), 1e-10, 1 - 1e-10) #logreg for observation n when actual = 1\n",
    "    right = np.clip((1 - y[n]) * np.log(1 - sigmoid(w.T @ X[n])), 1e-10, 1 - 1e-10) #logreg for observation n when actual = 0\n",
    "    return -sum(left + right)\n",
    "\n",
    "def grad_error_SGD(X, y, w, n):\n",
    "    return (sigmoid(w.T @ X[n]) - y[n]) * X[n] #gradient of logit for one observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4710ca91-a893-4bd4-a00e-9c5e0eb648a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regresssion check\n",
    "lgs = LogisticRegression(fit_intercept=False, solver='lbfgs')\n",
    "lgs.fit(X, y)\n",
    "beta_lgs = lgs.coef_.flatten()  # shape match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c6333-142d-4473-8786-630c499dad50",
   "metadata": {},
   "source": [
    "# Gradient Descent with Backtracking Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "45ea8efc-11b5-43ce-9f8f-f4d0e84cefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(X, y, eta, tol, epsilon, tau, max_iter=1000):\n",
    "    n, p = X.shape\n",
    "    beta = np.random.randn(p) * 0.01\n",
    "    i = 0\n",
    "    initial_eta = eta  # Save original eta\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        i += 1\n",
    "        grad = grad_error(X, y, beta)\n",
    "        eta = initial_eta  # Reset eta at the start of each iteration\n",
    "        beta_new = beta - eta * grad\n",
    "\n",
    "        while error(X, y, beta_new) > error(X, y, beta) - epsilon * eta * np.sum(grad**2): #check\n",
    "            eta *= tau\n",
    "            if eta < 10e-3: #accounts for if eta shrinks too much\n",
    "                break\n",
    "            beta_new = beta - eta * grad\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            break\n",
    "        \n",
    "        beta = beta_new\n",
    "\n",
    "    return beta_new, i\n",
    "\n",
    "beta_bls, iter_bls = backtracking(X, y, 1, 1e-6, 0.1, 0.8, 1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946644e-ac45-4fba-b39b-5a08e5f9d52f",
   "metadata": {},
   "source": [
    "The step size ($\\eta$) used should be a large step size. $\\epsilon$ and $\\tau$ are chosen from $(0, 1)$ but to prevent vanishing gradient by the backtracking algorithm, I set $\\epsilon = 0.1$ and $\\tau = 0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7d510a97-c9d5-4e7a-8736-d03d0c8f5e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38511601688783237\n"
     ]
    }
   ],
   "source": [
    "e1 = np.linalg.norm(beta_lgs - beta_bls)\n",
    "print(e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bec37-460b-4ff6-8138-28313e6f3d09",
   "metadata": {},
   "source": [
    "# Nesterov Gradient Descent with Backtracking Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1e7cf41b-e11e-4d16-978a-1dad4b7a4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(X, y, eta, tol, epsilon, tau, xi, max_iter=1000):\n",
    "    n, p = X.shape\n",
    "    beta_first = np.random.randn(p) * 0.01\n",
    "    i = 0\n",
    "    initial_eta = eta  # Save original eta\n",
    "    beta_second = beta_first - eta * grad_error(X, y, beta_first)\n",
    "\n",
    "    for _ in range(1, max_iter):\n",
    "        i += 1\n",
    "        lookahead = beta_second + xi * (beta_second - beta_first)\n",
    "        grad = grad_error(X, y, lookahead)\n",
    "        eta = initial_eta  # Reset eta at the start of each iteration\n",
    "        beta_new = lookahead - eta * grad\n",
    "\n",
    "        while error(X, y, beta_new) > error(X, y, lookahead) - epsilon * eta * np.sum(grad**2): #check\n",
    "            eta *= tau\n",
    "            beta_new = lookahead - eta * grad\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            break\n",
    "        \n",
    "        beta_first = beta_second\n",
    "        beta_second = beta_new\n",
    "\n",
    "    return beta_new, i\n",
    "\n",
    "beta_nesterov, iter_nesterov = nesterov(X, y, 1, 1e-6, 1e-2, 0.5, 0.5, 1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5dbb0e-592e-429d-9b31-ab5660459da4",
   "metadata": {},
   "source": [
    "The parameters in the armijo's condition are the same as the algorithm coded above. The momentum parameter $\\xi \\in (0, 1)$ will be chosen as $0.5$ to keep it in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "910da372-8f6b-43a4-922a-3f17364b0bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33241061892955603\n"
     ]
    }
   ],
   "source": [
    "e2 = np.linalg.norm(beta_nesterov - beta_lgs)\n",
    "print(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a6262-a908-4310-a666-6d62f3e8ed01",
   "metadata": {},
   "source": [
    "# AMSGrad-ADAM Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "67728d48-bf2d-4025-b667-4381e4281668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amsgrad_adam(X, y, eta, beta1=0.9, beta2=0.999, epsilon=1e-8, max_iter=1000, tol = 1e-6):\n",
    "    x = np.zeros(X.shape[1])\n",
    "    m = np.zeros_like(x)  #moment m\n",
    "    z = np.zeros_like(x)  #moment z\n",
    "    z_hat = z #z_hat to compare for max\n",
    "    i = 0\n",
    "    \n",
    "    for k in range(1, max_iter + 1):\n",
    "        i += 1 #iteration\n",
    "        g = grad_error(X, y, x)  #Compute gradient Gk\n",
    "\n",
    "        #Update m and z\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        z = beta2 * z + (1 - beta2) * (g**2)\n",
    "\n",
    "        #m_hat\n",
    "        m_hat = m / (1 - beta1 ** k)\n",
    "\n",
    "        #find max of z_hat and z\n",
    "        z_hat = np.maximum(z_hat, z)\n",
    "\n",
    "        # Normalize z\n",
    "        z_tilde = 1 / (np.sqrt(z_hat) + epsilon)\n",
    "\n",
    "        # Update parameters\n",
    "        x_new = x - eta * (z_tilde * m_hat)\n",
    "        \n",
    "        if np.linalg.norm(x_new - x) < tol:\n",
    "            break\n",
    "        x = x_new\n",
    "\n",
    "\n",
    "    return x, iter\n",
    "\n",
    "beta_ams, iter_ams = amsgrad_adam(X, y, eta = 0.01, beta1 = 0.9, beta2 = 0.99, epsilon = 1e-7, tol = 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c005194-cf8b-42b5-be1c-fccb934f9db0",
   "metadata": {},
   "source": [
    "I set the step size $\\eta = 0.01$ as this algorithm needs a small initial step size. Since $\\beta_2 \\in (0, 1)$ and $\\beta_1 \\in (0, \\beta_2)$, I initially chose $\\beta_2$ to be closer to 1 to emphasize the momentums and similarly for $\\beta_1$ but less than $\\beta_2$ to keep satisfy the restrictions. $\\epsilon$ is set to a small value to prevent vanishing gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8aff4785-f252-4132-ac41-d4de21cdc9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013357059687899048\n"
     ]
    }
   ],
   "source": [
    "e3 = np.linalg.norm(beta_lgs - beta_ams)\n",
    "print(e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4788a6-7e13-4140-a62e-bc88d8096e0c",
   "metadata": {},
   "source": [
    "# SGD with Decreasing Step Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ff07dbae-e224-4449-895d-f52d2802db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(X, y, batchsize, iter = 1000, step = 0.01, scale = 0.0001, tol = 1e-6):\n",
    "    #initialize beta0\n",
    "    n, p = X.shape\n",
    "    beta = np.random.randn(p) * 0.01\n",
    "    i = 0\n",
    "\n",
    "    for _ in range(iter):\n",
    "        #set the diminishing step size???\n",
    "        i += 1\n",
    "        step_new = step /  (1 + scale * i)\n",
    "        #set the error term from the minibatch\n",
    "        batch = np.random.randint(0, n - 1, batchsize)\n",
    "        target = 0\n",
    "        for j in batch:\n",
    "            target += grad_error_SGD(X, y, beta, j) #update error on summation of batch\n",
    "        target = target / batchsize\n",
    "\n",
    "        beta_new = beta - (step_new * target)\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            break\n",
    "        beta = beta_new\n",
    "\n",
    "    \n",
    "\n",
    "    return beta, i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba7bd8-ab34-4ff7-ba8a-300eec14851e",
   "metadata": {},
   "source": [
    "Batch size is determined by the given values the homework assigns. The step size is set to a small value to ensure convergence as the gradient can vary based on which observations are chosen for the SGD iteration. The scale for the diminishing step size is also set to a small value to ensure the step size isn't too small, which could lead to a vanishing gradient if it is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "184cc72c-a095-46bf-9d58-364c45a1295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size = 100\n",
    "beta_mini_100, iter_mini_100 = minibatch(X, y, batchsize = 100, step = 0.1, iter = 1000)\n",
    "\n",
    "# Batch Size = 200\n",
    "beta_mini_200, iter_mini_200 = minibatch(X, y, batchsize = 200, step = 0.1, iter = 1000)\n",
    "\n",
    "# Batch Size = 500\n",
    "beta_mini_500, iter_mini_500 = minibatch(X, y, batchsize = 500, step = 0.1, iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d540bf11-255c-4b9e-bd15-f0b504df4e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19096485542772365\n",
      "0.16441518898455296\n",
      "0.09662314721147974\n"
     ]
    }
   ],
   "source": [
    "# Batch size = 100\n",
    "e4 = np.linalg.norm(beta_mini_100 - beta_lgs)\n",
    "print(e4)\n",
    "\n",
    "# Batch size = 200\n",
    "e5 = np.linalg.norm(beta_mini_200 - beta_lgs)\n",
    "print(e5)\n",
    "\n",
    "# Batch Size = 500\n",
    "e6 = np.linalg.norm(beta_mini_500 - beta_lgs)\n",
    "print(e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fe1f4-9eff-4eac-8b2a-7e5a51d998d5",
   "metadata": {},
   "source": [
    "# SGD AMSGrad-ADAM Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f1553f0-c8c0-401f-8776-f00155113713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw(X, y, eta, batchsize, beta1=0.9, beta2=0.999, epsilon=1e-8, lmb = 0.1, max_iter=1000, tol = 1e-6):\n",
    "    n, p = X.shape\n",
    "    x = np.zeros(p)\n",
    "\n",
    "    m = np.zeros_like(x)  #moment m\n",
    "    z = np.zeros_like(x)  #moment z\n",
    "    z_hat = z #z_hat to compare for max\n",
    "    i = 0\n",
    "    \n",
    "    for k in range(1, max_iter + 1):\n",
    "        i += 1 #iteration\n",
    "        #set the error term from the minibatch\n",
    "        batch = np.random.randint(0, n - 1, batchsize)\n",
    "        g = 0\n",
    "        for j in batch:\n",
    "            g += grad_error_SGD(X, y, x, j) #update error on summation of batch\n",
    "        g = g/batchsize\n",
    "\n",
    "        #Update m and z\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        z = beta2 * z + (1 - beta2) * (g * g)\n",
    "\n",
    "        #m_hat\n",
    "        m_hat = m / (1 - beta1 ** k)\n",
    "\n",
    "        #find max of z_hat and z\n",
    "        z_hat = np.maximum(z_hat, z)\n",
    "\n",
    "        # Normalize z\n",
    "        z_tilde = 1 / (np.sqrt(z_hat) + epsilon)\n",
    "\n",
    "        # Update parameters\n",
    "        x = (1 - lmb*eta)*x - eta * (z_tilde * m_hat)\n",
    "        \n",
    "        #if np.linalg.norm(x\n",
    "\n",
    "\n",
    "    return x, iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9586cf-be7a-48a7-a201-0e37106a4229",
   "metadata": {},
   "source": [
    "All the parameters are similar to the parameters from SGD and AMSGrad-Adam. The only new parameter is $\\lambda$, the scale size for the weight decay. From the slides, the $\\lambda$ value should be set up to $10^{-2}$ as it is a small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "43178502-6ed2-42b8-b300-918f2a65ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_13140\\3475053029.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# Batch Size = 100\n",
    "beta_adamw_100, iter_ams_100 = adamw(X, y, eta = 0.1, batchsize = 100)\n",
    "\n",
    "# Batch Size = 200\n",
    "beta_adamw_200, iter_ams_200 = adamw(X, y, eta = 0.1, batchsize = 200)\n",
    "\n",
    "# Batch Size = 500\n",
    "beta_adamw_500, iter_ams_500 = adamw(X, y, eta = 0.1, batchsize = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8cad6531-9061-4de2-97d8-385287e6fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7793445616362327\n",
      "0.7153749717658412\n",
      "0.44986693439729647\n"
     ]
    }
   ],
   "source": [
    "# Batch size = 100\n",
    "e7 = np.linalg.norm(beta_adamw_100 - beta_lgs)\n",
    "print(e7)\n",
    "\n",
    "# Batch size = 200\n",
    "e8 = np.linalg.norm(beta_adamw_200 - beta_lgs)\n",
    "print(e8)\n",
    "\n",
    "# Batch Size = 500\n",
    "e9 = np.linalg.norm(beta_adamw_500 - beta_lgs)\n",
    "print(e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeca563-a54e-424b-9740-83df4cbca521",
   "metadata": {},
   "source": [
    "# Table of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "95a7cce9-58e9-46b7-9761-0294b5a2317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Estimation Error  Iterations\n",
      "Backtracking Line Search          0.385116        1000\n",
      "Nesterov BLS                      0.332411         999\n",
      "AMSGrad-ADAM                      0.001336        1000\n",
      "SGD 100                           0.190965        1000\n",
      "SGD 200                           0.164415        1000\n",
      "SGD 500                           0.096623        1000\n",
      "AMSGrad-ADAMw 100                 0.779345        1000\n",
      "AMSGrad-ADAMw 200                 0.715375        1000\n",
      "AMSGrad-ADAMw 500                 0.449867        1000\n"
     ]
    }
   ],
   "source": [
    "errors = [e1, e2, e3, e4, e5, e6, e7, e8, e9]\n",
    "iterations = [iter_bls, iter_nesterov, iter_ams, iter_mini_100, iter_mini_200, iter_mini_500, iter_ams_100, iter_ams_200, iter_ams_500]\n",
    "\n",
    "# Create Index\n",
    "index_names = [\n",
    "    \"Backtracking Line Search\",\n",
    "    \"Nesterov BLS\",\n",
    "    \"AMSGrad-ADAM\",\n",
    "    \"SGD 100\",\n",
    "    \"SGD 200\",\n",
    "    \"SGD 500\",\n",
    "    \"AMSGrad-ADAMw 100\",\n",
    "    \"AMSGrad-ADAMw 200\",\n",
    "    \"AMSGrad-ADAMw 500\"\n",
    "]\n",
    "\n",
    "# Create table\n",
    "table = pd.DataFrame({\n",
    "    'Estimation Error': errors,\n",
    "    'Iterations': iterations\n",
    "}, index = index_names)\n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93aac5-94e2-4e4a-b116-c39f328b28c8",
   "metadata": {},
   "source": [
    "- AMSGrad-ADAM seems to perform the best in my algorithm while AMSGrad-ADAMw 100 seems to perform the worst\n",
    "- The batch size may not be enough for the AMSGrad-ADAMw algorithm as the algorithm may be highly sensitive to the gradients the batch size selects as these gradients determine the weights\n",
    "- Maybe the weight decay is too small but wouldn't want to adjust as it may end up overfitting the data\n",
    "- The Nesterov BLS algorithm took the longest to run, while the SGD 100 took the shortest\n",
    "- I had to cap the eta for my BLS algorithm as it kept shrinking too fast, leaving my gradient to vanish and my algorithm to converge too fast\n",
    "- Many of my hyperparameters may be fine tuned to get a better estimation error but with runtime conflicts and the information given during lecture, these were the values I was able to get\n",
    "- I capped all iterations to 1000 and all the algorithms seem to require the max iterations\n",
    "- Overall, I understand why ADAM operation is widely used for neural networks, it had a decently fast convergence rate compared to the other algorithms and performed the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3542c-7566-4f3d-94f6-d38226e06d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f772d-a015-4aad-850d-bec2d69da1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94dee81-e0ac-4aa1-b831-142db9ad3b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
